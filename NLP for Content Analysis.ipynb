{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186945e2",
   "metadata": {},
   "source": [
    "# NLP for Content Analysis\n",
    "\n",
    "In this workshop, we will explore how to use Natural Language Processing (NLP) techniques to analyze and extract insights from text data. We will cover various NLP tasks such as tokenization, part-of-speech tagging, named entity recognition, and sentiment analysis.\n",
    "\n",
    "## Why NLP?\n",
    "\n",
    "NLP is a powerful tool for understanding and processing human language. There are many situations where the quantity of text data available is too large for manual analysis, and NLP can help automate the extraction of useful information. For example, we can use NLP to analyze policy documents to identify key themes, extract named entities such as organizations and locations, and assess the sentiment of the text.\n",
    "\n",
    "## Our Tools\n",
    "\n",
    "In this workshop, we'll focus on two useful tools for NLP: spaCy, and the Hugging Face Transformers library. Both libraries provide powerful and efficient implementations of various NLP tasks, making it easy to get started with text analysis.\n",
    "\n",
    "## spaCy\n",
    "\n",
    "[spaCy](https://spacy.io/) is an open-source library for advanced NLP in Python. It is designed specifically for large-scale use and provides a fast and efficient way to process large volumes of text. spaCy includes pre-trained models for various languages, which can be used for tasks such as tokenization, part-of-speech tagging, named entity recognition, and dependency parsing.\n",
    "\n",
    "If it's not already installed, you can install spaCy using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aab28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44915393",
   "metadata": {},
   "source": [
    "spaCy works with pre-trained models for various languages. We'll work today with the small English model, which is suitable for learning. To install the small English model, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e78213",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5fdd27",
   "metadata": {},
   "source": [
    "Once successfully installed, we can load spaCy and the English model in our Python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82d800",
   "metadata": {},
   "source": [
    "We can now use the `nlp` object to process text. For example, let's analyze a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = nlp(\"The quick brown fox jumps over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8fb095",
   "metadata": {},
   "source": [
    "We can now make use of our processed `document` object to understand the text better. For example, we can access the `tokens` in the document, which are the individual words and punctuation marks broken down by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Token\": <20}{\"Reduced Form (aka lemma)\": <20}')\n",
    "for token in document:\n",
    "    print(f\"{token.text: <20}{token.lemma_: <20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa260c",
   "metadata": {},
   "source": [
    "Notice how in this example, the word \"jumps\" is reduced to its lemma, \"jump\". This is a common NLP technique called lemmatization, which reduces words to their base or dictionary form. This can be useful for tasks such as text classification or sentiment analysis, where we want to treat different forms of a word as the same. \n",
    "\n",
    "### Exercise 1\n",
    "Try coming up with a complex sentence and see how spaCy processes it. You can use the method below, changing the sentence. Try some of the following:\n",
    "- Complex compound words\n",
    "- Words in different tenses\n",
    "- Words with different prefixes or suffixes\n",
    "- Made up words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654555d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    print(f'{\"Token\": <20}{\"Lemma\": <20}')\n",
    "    for token in doc:\n",
    "        print(f\"{token.text: <20}{token.lemma_: <20}\")\n",
    "\n",
    "lemmatize_sentence(\"Put your complex sentence here, with different tenses, prefixes, and suffixes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d6b50",
   "metadata": {},
   "source": [
    "With spaCy, we can also perform more advanced tasks. For example, spaCy makes a prediction about the part of speech of each token in the document. This can be useful if we only care about certain parts of speech, such as nouns or verbs. We can access the part of speech tags using the `pos_` attribute of each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d613e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tags(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    print(f'{\"Token\": <20}{\"Part of Speech\": <20}')\n",
    "    for token in doc:\n",
    "        print(f\"{token.text: <20}{token.pos_: <20}\")\n",
    "\n",
    "pos_tags(\"The quick brown fox jumps over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e514dbf",
   "metadata": {},
   "source": [
    "For a complete list of part-of-speech tags and their meanings, you can refer to the [Universal POS Tags](https://universaldependencies.org/u/pos/).\n",
    "\n",
    "Let's go ahead and try a worked example with a real (if small) dataset. For this part, we'll use the [Hugging Face Datasets library](https://huggingface.co/docs/datasets/index) to load the `AI Jobs News Articles` dataset, which contains news articles related to AI jobs. We'll use spaCy to analyze the text in these articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d8bad2",
   "metadata": {},
   "source": [
    "The code below will download the dataset and show us the first few examples.\n",
    "\n",
    "**NOTE:** If at any point in the lab you find that the code is running too slowly, you can adjust the `download_percent` variable below to a lower value. This will reduce the number of articles loaded, which can speed up processing time. However, for the full analysis, we recommend using the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "download_percent = 100 # Adjust this to a lower value (e.g., 10) if you want to load a smaller subset of the dataset\n",
    "\n",
    "# Load a small dataset of news articles related to AI jobs\n",
    "ds = load_dataset(\"fdaudens/ai-jobs-news-articles\", split=f'train[:{download_percent}%]')\n",
    "\n",
    "# Display the first few articles\n",
    "for i in range(3):\n",
    "    article = ds[i]\n",
    "    print(f\"Title: {article['title']}\")\n",
    "    print(f\"Content: {article['text'][:512]}...\")  # Display first 512 characters\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856a1e1",
   "metadata": {},
   "source": [
    "There are many different questions we might want to ask of this dataset, but let's start with a simple one: what are the most common words in the articles? We can use spaCy to tokenize the text and then count the frequency of each token. Because we can `lemmatize` the tokens, we can be sure that we are counting the base form of each word, rather than different forms of the same word (e.g., \"run\" and \"running\" will both be counted as \"run\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec48106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_tokens_pipe(articles, batch_size=100):\n",
    "    token_counter = Counter()\n",
    "\n",
    "    # Remove any articles that do not have text\n",
    "    texts = [article['text'] for article in articles if article.get('text') is not None]\n",
    "\n",
    "    # Process the texts in batches to avoid memory issues\n",
    "    for doc in tqdm(nlp.pipe(texts, disable=[\"ner\", \"parser\"], batch_size=batch_size), total=len(texts)):\n",
    "        for token in doc:\n",
    "\n",
    "            # Check if the token is not a stop word, punctuation, or empty lemma\n",
    "            if not token.is_stop and not token.is_punct and len(token.lemma_.strip()) > 0:\n",
    "                token_counter[token.lemma_] += 1\n",
    "\n",
    "    return token_counter\n",
    "\n",
    "# Count tokens in the dataset\n",
    "token_counts = count_tokens_pipe(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291345c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the 10 most common tokens\n",
    "\n",
    "print(\"\\nMost common tokens:\")\n",
    "for token, count in token_counts.most_common(10):\n",
    "    print(f\"{token}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fbbcb7",
   "metadata": {},
   "source": [
    "These results are not particularly surprising, but they do give a sense of the most common words in the dataset. As we'd expect, words like \"AI\", \"job\" and \"skill\" are among the most common, as they are central to the topic of the articles. Let's move on to a more complex analysis.\n",
    "\n",
    "## Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition (NER) is a key NLP task that involves identifying and classifying named entities in text into predefined categories such as persons, organizations, locations, dates, and more. NER can be particularly useful for extracting structured information from unstructured text data.\n",
    "\n",
    "In this dataset, many articles include references to specific people in the world of AI. We can use spaCy's NER capabilities to identify people mentioned in the articles. Going beyond that, let's say that we want to evaluate whether the articles are positive, negative, or neutral in tone. We can use spaCy's sentiment analysis capabilities to do this.\n",
    "\n",
    "Putting these concepts together, we can create a function that allows us to estimate the sentiment surrounding each person mentioned in the articles. This will give us a sense of how people in the AI field are perceived in the news articles.\n",
    "\n",
    "We do need one additional library to make this work, called spacytextblob. This library provides a simple interface for sentiment analysis using the TextBlob library, which is built on top of NLTK and provides a simple API for common NLP tasks. Let's go ahead and install that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca42264",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq spacytextblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa561039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "if \"spacytextblob\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"spacytextblob\", last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece611b",
   "metadata": {},
   "source": [
    "Our first job is to extract the named entities from the articles. Named Entity Recognition identifies more than just poeple; it can also identify organizations, locations, dates and more. For our purposes, we'll first task spaCy with identifying named entities, and then we'll check if they're of type \"PERSON\". If they are, we'll add them to a list of people mentioned in the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_people(texts, batch_size: int = 64):\n",
    "    \"\"\"\n",
    "    Returns a list (len == n_texts) of PERSON-name lists.\n",
    "    \"\"\"\n",
    "    people_per_doc = []\n",
    "    for doc in tqdm(\n",
    "        nlp.pipe(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            disable=[\"parser\", \"tagger\", \"attribute_ruler\",\n",
    "                     \"lemmatizer\", \"spacytextblob\"],   # keep only NER\n",
    "        ),\n",
    "        total=len(texts),\n",
    "        desc=\"Finding PERSON entities\",\n",
    "    ):\n",
    "        names = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "        people_per_doc.append(names)\n",
    "    return people_per_doc\n",
    "\n",
    "extract_people([\"Sam Altman is the CEO of OpenAI.\", \"At the recent AI conference, Professor Geoffrey Hinton spoke\", \"We caught up with Sundar Pichai, the CEO of Google.\"], batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae06410",
   "metadata": {},
   "source": [
    "Great, we can see that the function works as expected. It returns a list of people mentioned in the articles, which we can then use for further analysis.\n",
    "\n",
    "Our next step is to analyze the sentiment of the articles.\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is the process of determining the emotional tone behind a series of words. It is often used to understand the sentiment expressed in text data, such as social media posts, product reviews, or news articles. Automated prediction of sentiment has come a long way in recent years, and modern NLP libraries like spaCy make it easy to perform sentiment analysis on text data.\n",
    "\n",
    "To perform sentiment analysis with spaCy, we can use the `spacytextblob` library, which provides a simple interface for sentiment analysis using the TextBlob library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4881703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentiment(texts, batch_size: int = 64):\n",
    "    \"\"\"\n",
    "    Returns a list (len == n_texts) of polarity scores in [-1, 1].\n",
    "    \"\"\"\n",
    "    polarities = []\n",
    "    for doc in tqdm(\n",
    "        nlp.pipe(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            disable=[\"parser\", \"tagger\", \"attribute_ruler\",\n",
    "                     \"lemmatizer\", \"ner\"],             # keep only spacytextblob\n",
    "        ),\n",
    "        total=len(texts),\n",
    "        desc=\"Scoring sentiment\",\n",
    "    ):\n",
    "        polarities.append(doc._.blob.polarity)\n",
    "    return polarities\n",
    "\n",
    "score_sentiment([\"I love AI!\", \"AI is terrible.\", \"I have no opinion on AI.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe187b81",
   "metadata": {},
   "source": [
    "There are more sophisticated methods for sentiment analysis, but for our purposes, this will work well. The `spacytextblob` library provides a simple way to get the sentiment polarity of a text, which ranges from -1 (very negative) to 1 (very positive). A polarity of 0 indicates a neutral sentiment.\n",
    "\n",
    "Now that we have our NER and sentiment analysis functions, we can start analyzing the articles in more depth. We can extract the people mentioned in each article and their associated sentiment scores, allowing us to see not only who is being discussed but also the sentiment surrounding those individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_person_sentiment_df(articles, text_key: str = \"text\"):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame:  article_id | person | polarity\n",
    "    \"\"\"\n",
    "    # --- collect texts & IDs (skip empty ones) ---\n",
    "    texts = [a[text_key] for a in articles if a.get(text_key)]\n",
    "    article_ids = [i for i, a in enumerate(articles) if a.get(text_key)]\n",
    "\n",
    "    # --- run the two independent passes ---\n",
    "    people_lists = extract_people(texts)\n",
    "    polarity_list = score_sentiment(texts)\n",
    "\n",
    "    # --- explode to long form ---\n",
    "    records = []\n",
    "    for art_id, names, pol in zip(article_ids, people_lists, polarity_list):\n",
    "        for name in names:\n",
    "            records.append({\"article_id\": art_id, \"person\": name, \"polarity\": pol})\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ebc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = build_person_sentiment_df(ds, text_key=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a2418",
   "metadata": {},
   "source": [
    "Now we have a dataset listing every mention of a person, and what polarity the article is predicted to be. We can now use this to compute the average for each person, as well as how many times they were mentioned. This will give us a sense of how often people are mentioned in the articles, and how positive or negative the sentiment is towards them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "people.groupby('person').agg(\n",
    "    mentions=('polarity', 'count'),\n",
    "    avg_sentiment=('polarity', 'mean')\n",
    ").reset_index().sort_values(by='mentions', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a25d97",
   "metadata": {},
   "source": [
    "As we can see, there are clearly many entities picked up by the model that are in actuality not people. Keep in mind that we are using the smallest English model, which is significantly less accurate than the larger models. In order to improve performance, let's switch to the HuggingFace Transformers library, which provides access to state-of-the-art NLP models.\n",
    "\n",
    "\n",
    "## Hugging Face Transformers\n",
    "\n",
    "The [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) library provides a wide range of pre-trained models for various NLP tasks, including named entity recognition, sentiment analysis, and more. It is built on top of PyTorch and TensorFlow, making it easy to use with both frameworks. HuggingFace has particularly become the go-to resource for transformer-based models, which is the technology behind Large Language Models (LLMs) like GPT.\n",
    "\n",
    "In this case, we won't be working with a full LLM, since this is too resource-intensive to run on a personal computer. Instead, we'll use pre-trained models that are specifically designed for tasks like NER and sentiment analysis. These models have been trained on large datasets and can be fine-tuned for specific tasks.\n",
    "\n",
    "### Named Entity Recognition with Hugging Face\n",
    "\n",
    "For our identification of peoples' names, we will use the `distilbert-NER` model. BERT is an early success in the field of transformers, and DistilBERT is a smaller, faster, cheaper version of BERT that retains 97% of its language understanding while being 60% faster and using 40% less memory. As the name implies, this version of distilBERT is specifically trained for NER tasks, and is a good fit for our needs.\n",
    "\n",
    "Let's make sure we have the necessary libraries installed, and then we can load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dslim/distilbert-NER\",\n",
    "    aggregation_strategy=\"max\"\n",
    ")\n",
    "\n",
    "def extract_people(\n",
    "    texts,\n",
    "    batch_size: int = 16,\n",
    "    stride: int = 128,\n",
    "    show_progress: bool = True\n",
    "):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    persons_per_doc = []\n",
    "    for start in tqdm(\n",
    "        range(0, len(texts), batch_size),\n",
    "        desc=\"Performing NER\",\n",
    "        unit=\"doc\",\n",
    "        disable=not show_progress\n",
    "    ):\n",
    "        batch = texts[start : start + batch_size]\n",
    "        batch_entities = ner_pipeline(\n",
    "            batch,\n",
    "            batch_size=len(batch),\n",
    "            stride=stride\n",
    "        )\n",
    "        for doc in batch_entities:\n",
    "            persons_per_doc.append(\n",
    "                [e[\"word\"].strip() for e in doc if e[\"entity_group\"] == \"PER\" and len(e[\"word\"].strip()) > 5]\n",
    "            )\n",
    "\n",
    "    return persons_per_doc\n",
    "\n",
    "\n",
    "# ─── quick demo ────────────────────────────────────────────────────────────────\n",
    "docs = [\n",
    "    \"Sam Altman is the CEO of OpenAI.\",\n",
    "    \"Professor Geoffrey Hinton spoke at the conference.\",\n",
    "    \"We caught up with Sundar Pichai, the CEO of Google.\"\n",
    "]\n",
    "print(extract_people(docs, batch_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12fb9ca",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Hugging Face\n",
    "\n",
    "For sentiment analysis, we will use the `distilroberta-finetuned-financial-news-sentiment-analysis` model. This model is fine-tuned for sentiment analysis on financial news articles, which makes it a good fit for our dataset of AI job news articles. It is based on the RoBERTa architecture, which is a variant of BERT that has been shown to perform well on a variety of NLP tasks. It is distilled, like DistilBERT, to be smaller and faster while retaining much of the original model's performance.\n",
    "\n",
    "Let's load the sentiment analysis pipeline and define a function to extract sentiment from text. For longer texts, we will use a sliding window approach to ensure that we can process the entire text without exceeding the model's maximum input length. This is particularly important for our dataset, which contains articles that can be quite long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc1917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\", truncation=True)\n",
    "\n",
    "def _score_long_text(text, max_len=512, stride=128, batch_size=16):\n",
    "    \"\"\"\n",
    "    Return (label, score) for a single *text* of arbitrary length.\n",
    "    \"\"\"\n",
    "    tk = sentiment_pipeline.tokenizer\n",
    "\n",
    "    # a) Split into overlapping windows\n",
    "    enc = tk(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=False\n",
    "    )\n",
    "    chunks = tk.batch_decode(enc[\"input_ids\"], skip_special_tokens=True)\n",
    "\n",
    "    # b) Run the model on each window\n",
    "    chunk_preds = sentiment_pipeline(chunks, batch_size=batch_size)\n",
    "\n",
    "    # c) Aggregate: signed average of probabilities\n",
    "    signed = [\n",
    "        p[\"score\"] * (+1 if p[\"label\"].lower() == \"positive\"\n",
    "                      else -1 if p[\"label\"].lower() == \"negative\"\n",
    "                      else 0)\n",
    "        for p in chunk_preds\n",
    "    ]\n",
    "    avg = np.mean(signed)\n",
    "    return avg\n",
    "\n",
    "# 3 ─── Public function that handles a list or single string\n",
    "def extract_sentiment(texts, batch_size: int = 16,\n",
    "                      max_len: int = 512, stride: int = 128):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    return [\n",
    "        _score_long_text(t, max_len=max_len, stride=stride,\n",
    "                         batch_size=batch_size)\n",
    "        for t in tqdm(texts, desc=\"Scoring sentiment\", unit=\"doc\")\n",
    "    ]\n",
    "\n",
    "\n",
    "extract_sentiment([\n",
    "    \"The job market for AI professionals is booming.\",\n",
    "    \"Investors are worried about rising interest rates.\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_person_sentiment_df(articles, text_key: str = \"text\"):\n",
    "    # Collect non-empty texts and remember their original indices\n",
    "    texts, art_ids = [], []\n",
    "    for i, art in enumerate(articles):\n",
    "        if art.get(text_key):  # skip empty / missing text, and limit to 512 characters\n",
    "            texts.append(art[text_key])\n",
    "            art_ids.append(i)\n",
    "\n",
    "    people_per_article    = extract_people(texts)\n",
    "    sentiment_per_article = extract_sentiment(texts)\n",
    "\n",
    "    records = []\n",
    "    for art_id, names, score in zip(\n",
    "        art_ids, people_per_article, sentiment_per_article\n",
    "    ):\n",
    "        for name in names:\n",
    "            records.append(\n",
    "                {\n",
    "                    \"article_id\": art_id,\n",
    "                    \"person\":     name,\n",
    "                    \"score\":      score\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "people = build_person_sentiment_df(ds, text_key=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5dd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "people.groupby('person').agg(\n",
    "    mentions=('score', 'count'),\n",
    "    avg_sentiment=('score', 'mean')\n",
    ").reset_index().sort_values(by='mentions', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8477c71",
   "metadata": {},
   "source": [
    "Our results for this example are much better than with the small English model in spaCy. Although we still have some false positives, the model is much better at identifying people.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In this workshop, we have explored how to use NLP techniques to analyze and extract insights from text data. We have covered various NLP tasks such as tokenization, part-of-speech tagging, named entity recognition, and sentiment analysis using spaCy and Hugging Face Transformers. We showed how to apply these techniques to a real-world dataset of AI job news articles, extracting named entities and sentiment scores for individuals mentioned in the articles.\n",
    "\n",
    "#### Extension\n",
    "\n",
    "spaCy has many more features that we haven't covered in this notebook. If you still have time, take a look at the [spaCy documentation](https://spacy.io/usage) to learn more about what you can do with spaCy. Some interesting topics to explore include:\n",
    "- Dependency parsing: Understanding the grammatical structure of sentences.\n",
    "- Text classification: Automatically categorizing text into predefined categories.\n",
    "- Language models: Training your own models for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c817bbe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
